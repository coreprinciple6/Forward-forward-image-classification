{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nEnDSZQbqsv"
      },
      "source": [
        "## Setup imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "OAJQRx74bqsv"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report, accuracy_score\n",
        "from sklearn.utils import shuffle\n",
        "import random\n",
        "from tensorflow.compiler.tf2xla.python import xla\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "rand_seed = 42\n",
        "tf.random.set_seed(rand_seed)\n",
        "weights_base_path = \"../../data/weights\"\n",
        "images_base_path = \"../../data/plots\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzqFTWIxbqsw"
      },
      "source": [
        "## Load the dataset and visualize the data\n",
        "\n",
        "We use the `keras.datasets.mnist.load_data()` utility to directly pull the MNIST dataset\n",
        "in the form of `NumPy` arrays. We then arrange it in the form of the train and test\n",
        "splits.\n",
        "\n",
        "Following loading the dataset, we select 4 random samples from within the training set\n",
        "and visualize them using `matplotlib.pyplot`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "F_C1jQVDbqsw"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "shuffle() got an unexpected keyword argument 'seed'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m/Users/tristanleduc/Documents/TRISTAN/SCOLARITE/ECOLE_INGEÃÅNIEUR/IIT_CHICAGO/Cours/CS512-ComputerVision/Forward-forward-image-classification/code/Base model performance/MNIST/forwardforward_mnist.ipynb Cellule 5\u001b[0m in \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tristanleduc/Documents/TRISTAN/SCOLARITE/ECOLE_INGE%CC%81NIEUR/IIT_CHICAGO/Cours/CS512-ComputerVision/Forward-forward-image-classification/code/Base%20model%20performance/MNIST/forwardforward_mnist.ipynb#W5sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m (x_train, y_train), (x_test, y_test) \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39mdatasets\u001b[39m.\u001b[39mmnist\u001b[39m.\u001b[39mload_data()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/tristanleduc/Documents/TRISTAN/SCOLARITE/ECOLE_INGE%CC%81NIEUR/IIT_CHICAGO/Cours/CS512-ComputerVision/Forward-forward-image-classification/code/Base%20model%20performance/MNIST/forwardforward_mnist.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m x_train, y_train \u001b[39m=\u001b[39m shuffle(x_train, y_train, seed\u001b[39m=\u001b[39;49mrand_seed)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tristanleduc/Documents/TRISTAN/SCOLARITE/ECOLE_INGE%CC%81NIEUR/IIT_CHICAGO/Cours/CS512-ComputerVision/Forward-forward-image-classification/code/Base%20model%20performance/MNIST/forwardforward_mnist.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m x_test, y_test \u001b[39m=\u001b[39m shuffle(x_test, y_test, seed\u001b[39m=\u001b[39mrand_seed)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tristanleduc/Documents/TRISTAN/SCOLARITE/ECOLE_INGE%CC%81NIEUR/IIT_CHICAGO/Cours/CS512-ComputerVision/Forward-forward-image-classification/code/Base%20model%20performance/MNIST/forwardforward_mnist.ipynb#W5sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39m4 Random Training samples and labels\u001b[39m\u001b[39m\"\u001b[39m)\n",
            "\u001b[0;31mTypeError\u001b[0m: shuffle() got an unexpected keyword argument 'seed'"
          ]
        }
      ],
      "source": [
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "\n",
        "x_train, y_train = shuffle(x_train, y_train, random_state=rand_seed)\n",
        "x_test, y_test = shuffle(x_test, y_test, random_state=rand_seed)\n",
        "\n",
        "print(\"4 Random Training samples and labels\")\n",
        "idx1, idx2, idx3, idx4 = random.sample(range(0, x_train.shape[0]), 4)\n",
        "\n",
        "img1 = (x_train[idx1], y_train[idx1])\n",
        "img2 = (x_train[idx2], y_train[idx2])\n",
        "img3 = (x_train[idx3], y_train[idx3])\n",
        "img4 = (x_train[idx4], y_train[idx4])\n",
        "\n",
        "imgs = [img1, img2, img3, img4]\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "\n",
        "for idx, item in enumerate(imgs):\n",
        "    image, label = item[0], item[1]\n",
        "    plt.subplot(2, 2, idx + 1)\n",
        "    plt.imshow(image, cmap=\"gray\")\n",
        "    plt.title(f\"Label : {label}\")\n",
        "plt.show()\n",
        "\n",
        "class_names = np.asarray([str(i) for i in range(10)])\n",
        "\n",
        "true_labels = class_names[y_test]\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Performance metrics and visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_loss_curve(history, title, path_to_save=None):\n",
        "    plt.plot(history.history[\"FinalLoss\"], label=\"Training loss\")\n",
        "    plt.title(title)\n",
        "    plt.xlabel(\"Epochs\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "    if path_to_save:\n",
        "        plt.savefig(path_to_save)\n",
        "\n",
        "\n",
        "def load_eval_best_model(model, weights_path, test_images, true_labels, class_names, model_name):\n",
        "    model = FFNetwork(dims=[784, 500, 500])\n",
        "    model.load_weights(weights_path)\n",
        "    preds = model.predict(tf.convert_to_tensor(test_images))\n",
        "    preds = preds.reshape((preds.shape[0], preds.shape[1]))\n",
        "    pred_labels = class_names[np.argmax(preds, axis=1)]\n",
        "    cm = confusion_matrix(true_labels, pred_labels, labels=class_names)\n",
        "    disp = ConfusionMatrixDisplay(cm, display_labels=class_names)\n",
        "    fig, ax = plt.subplots()\n",
        "    fig.set_figheight(8)\n",
        "    fig.set_figwidth(8)\n",
        "\n",
        "    disp.plot(ax=ax, xticks_rotation='vertical', colorbar=False)\n",
        "    plt.title(\"Confusion matrix of the \" +\n",
        "              model_name + \" model on MNIST test set\")\n",
        "    plt.show()\n",
        "\n",
        "    acc = accuracy_score(true_labels, pred_labels)\n",
        "    print(f\"Test error rate: {1 - acc:.4f}\")\n",
        "\n",
        "    print(classification_report(true_labels, pred_labels, labels=class_names))\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "yWNIHPx9bqsw"
      },
      "source": [
        "## Define `FFDense` custom layer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i7KRglLWbqsx"
      },
      "outputs": [],
      "source": [
        "\n",
        "class FFDense(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    A custom ForwardForward-enabled Dense layer. It has an implementation of the\n",
        "    Forward-Forward network internally for use.\n",
        "    This layer must be used in conjunction with the `FFNetwork` model.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        units,\n",
        "        optimizer,\n",
        "        loss_metric,\n",
        "        num_epochs=50,\n",
        "        use_bias=True,\n",
        "        kernel_initializer=\"glorot_uniform\",\n",
        "        bias_initializer=\"zeros\",\n",
        "        kernel_regularizer=None,\n",
        "        bias_regularizer=None,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.dense = keras.layers.Dense(\n",
        "            units=units,\n",
        "            use_bias=use_bias,\n",
        "            kernel_initializer=kernel_initializer,\n",
        "            bias_initializer=bias_initializer,\n",
        "            kernel_regularizer=kernel_regularizer,\n",
        "            bias_regularizer=bias_regularizer,\n",
        "        )\n",
        "        self.relu = keras.layers.ReLU()\n",
        "        self.optimizer = optimizer\n",
        "        self.loss_metric = loss_metric\n",
        "        self.threshold = 1.5\n",
        "        self.num_epochs = num_epochs\n",
        "\n",
        "\n",
        "    def call(self, x):\n",
        "        x_norm = tf.norm(x, ord=2, axis=1, keepdims=True)\n",
        "        x_norm = x_norm + 1e-4\n",
        "        x_dir = x / x_norm\n",
        "        res = self.dense(x_dir)\n",
        "        return self.relu(res)\n",
        "\n",
        "\n",
        "    def forward_forward(self, x_pos, x_neg):\n",
        "        for i in range(self.num_epochs):\n",
        "            with tf.GradientTape() as tape:\n",
        "                g_pos = tf.math.reduce_mean(tf.math.pow(self.call(x_pos), 2), 1)\n",
        "                g_neg = tf.math.reduce_mean(tf.math.pow(self.call(x_neg), 2), 1)\n",
        "\n",
        "                loss = tf.math.log(\n",
        "                    1\n",
        "                    + tf.math.exp(\n",
        "                        tf.concat([-g_pos + self.threshold, g_neg - self.threshold], 0)\n",
        "                    )\n",
        "                )\n",
        "                mean_loss = tf.cast(tf.math.reduce_mean(loss), tf.float32)\n",
        "                self.loss_metric.update_state([mean_loss])\n",
        "            gradients = tape.gradient(mean_loss, self.dense.trainable_weights)\n",
        "            self.optimizer.apply_gradients(zip(gradients, self.dense.trainable_weights))\n",
        "        return (\n",
        "            tf.stop_gradient(self.call(x_pos)),\n",
        "            tf.stop_gradient(self.call(x_neg)),\n",
        "            self.loss_metric.result(),\n",
        "        )\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-AbYZliebqsx"
      },
      "source": [
        "## Define the `FFNetwork` Custom Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8DM9Dr5Vbqsx"
      },
      "outputs": [],
      "source": [
        "\n",
        "class FFNetwork(keras.Model):\n",
        "    \"\"\"\n",
        "    A `keras.Model` that supports a `FFDense` network creation. This model\n",
        "    can work for any kind of classification task. It has an internal\n",
        "    implementation with some details specific to the MNIST dataset which can be\n",
        "    changed as per the use-case.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dims,\n",
        "        layer_optimizer=keras.optimizers.legacy.Adam(learning_rate=0.03),\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.layer_optimizer = layer_optimizer\n",
        "        self.loss_var = tf.Variable(0.0, trainable=False, dtype=tf.float32)\n",
        "        self.loss_count = tf.Variable(0.0, trainable=False, dtype=tf.float32)\n",
        "        self.layer_list = [keras.Input(shape=(dims[0],))]\n",
        "        for d in range(len(dims) - 1):\n",
        "            self.layer_list += [\n",
        "                FFDense(\n",
        "                    dims[d + 1],\n",
        "                    optimizer=self.layer_optimizer,\n",
        "                    loss_metric=keras.metrics.Mean(),\n",
        "                )\n",
        "            ]\n",
        "\n",
        "    @tf.function(reduce_retracing=True)\n",
        "    def overlay_y_on_x(self, data):\n",
        "        X_sample, y_sample = data\n",
        "        max_sample = tf.reduce_max(X_sample, axis=0, keepdims=True)\n",
        "        max_sample = tf.cast(max_sample, dtype=tf.float64)\n",
        "        X_zeros = tf.zeros([10], dtype=tf.float64)\n",
        "        X_update = xla.dynamic_update_slice(X_zeros, max_sample, [y_sample])\n",
        "        X_sample = xla.dynamic_update_slice(X_sample, X_update, [0])\n",
        "        return X_sample, y_sample\n",
        "\n",
        "    @tf.function(reduce_retracing=True)\n",
        "    def predict_one_sample(self, x):\n",
        "        goodness_per_label = []\n",
        "        x = tf.reshape(x, [tf.shape(x)[0] * tf.shape(x)[1]])\n",
        "        for label in range(10):\n",
        "            h, label = self.overlay_y_on_x(data=(x, label))\n",
        "            h = tf.reshape(h, [-1, tf.shape(h)[0]])\n",
        "            goodness = []\n",
        "            for layer_idx in range(1, len(self.layer_list)):\n",
        "                layer = self.layer_list[layer_idx]\n",
        "                h = layer(h)\n",
        "                goodness += [tf.math.reduce_mean(tf.math.pow(h, 2), 1)]\n",
        "            goodness_per_label += [\n",
        "                tf.expand_dims(tf.reduce_sum(goodness, keepdims=True), 1)\n",
        "            ]\n",
        "        goodness_per_label = tf.concat(goodness_per_label, 1)\n",
        "        return tf.cast(tf.argmax(goodness_per_label, 1), tf.float64)\n",
        "\n",
        "    def predict(self, data):\n",
        "        x = data\n",
        "        preds = list()\n",
        "        preds = tf.map_fn(fn=self.predict_one_sample, elems=x)\n",
        "        return np.asarray(preds, dtype=int)\n",
        "\n",
        "    @tf.function(jit_compile=True)\n",
        "    def train_step(self, data):\n",
        "        x, y = data\n",
        "\n",
        "        # Flatten op\n",
        "        x = tf.reshape(x, [-1, tf.shape(x)[1] * tf.shape(x)[2]])\n",
        "\n",
        "        x_pos, y = tf.map_fn(fn=self.overlay_y_on_x, elems=(x, y))\n",
        "\n",
        "        random_y = tf.random.shuffle(y)\n",
        "        x_neg, y = tf.map_fn(fn=self.overlay_y_on_x, elems=(x, random_y))\n",
        "\n",
        "        h_pos, h_neg = x_pos, x_neg\n",
        "\n",
        "        for idx, layer in enumerate(self.layers):\n",
        "            if isinstance(layer, FFDense):\n",
        "                print(f\"Training layer {idx+1} now : \")\n",
        "                h_pos, h_neg, loss = layer.forward_forward(h_pos, h_neg)\n",
        "                self.loss_var.assign_add(loss)\n",
        "                self.loss_count.assign_add(1.0)\n",
        "            else:\n",
        "                print(f\"Passing layer {idx+1} now : \")\n",
        "                x = layer(x)\n",
        "        mean_res = tf.math.divide(self.loss_var, self.loss_count)\n",
        "        return {\"FinalLoss\": mean_res}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlDJVuS0bqsy"
      },
      "source": [
        "## Convert MNIST `NumPy` arrays to `tf.data.Dataset`\n",
        "\n",
        "We now perform some preliminary processing on the `NumPy` arrays and then convert them\n",
        "into the `tf.data.Dataset` format which allows for optimized loading."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GtXseaWgbqsy"
      },
      "outputs": [],
      "source": [
        "x_train = x_train.astype(float) / 255\n",
        "x_test = x_test.astype(float) / 255\n",
        "y_train = y_train.astype(int)\n",
        "y_test = y_test.astype(int)\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "\n",
        "train_dataset = train_dataset.batch(60000)\n",
        "test_dataset = test_dataset.batch(10000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjV3duuEbqsy"
      },
      "source": [
        "## Fit the network and visualize results\n",
        "\n",
        "Having performed all previous set-up, we are now going to run `model.fit()` and run 250\n",
        "model epochs, which will perform 50*250 epochs on each layer. We get to see the plotted loss\n",
        "curve as each layer is trained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ax63lgtBbqsy"
      },
      "outputs": [],
      "source": [
        "model = FFNetwork(dims=[784, 500, 500])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.03),\n",
        "    loss=\"mse\",\n",
        "    jit_compile=True,\n",
        "    metrics=[keras.metrics.Mean()],\n",
        ")\n",
        "\n",
        "epochs = 250\n",
        "history = model.fit(train_dataset, epochs=epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# save the weights of the model\n",
        "weights_path = os.path.join(weights_base_path, \"baseline_weights.h5\")\n",
        "model.save_weights(weights_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fD_rjLScbqsy"
      },
      "source": [
        "## Perform inference and testing\n",
        "\n",
        "Having trained the model to a large extent, we now see how it performs on the\n",
        "test set. We calculate the Accuracy Score to understand the results closely."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# preds = model.predict(tf.convert_to_tensor(x_test))\n",
        "\n",
        "# preds = preds.reshape((preds.shape[0], preds.shape[1]))\n",
        "\n",
        "# results = accuracy_score(preds, y_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RrXn8lQUbqsy"
      },
      "outputs": [],
      "source": [
        "\n",
        "plot_path = os.path.join(images_base_path, \"baseline-mnist.png\")\n",
        "plot_loss_curve(history,title=\"Baseline Model Loss Curve on MNIST Dataset\")\n",
        "\n",
        "model_name = \"baseline_model\"\n",
        "load_eval_best_model(model, weights_path, x_test, y_test,class_names, model_name)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "forwardforward",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
