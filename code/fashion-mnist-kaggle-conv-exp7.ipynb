{"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install tensorflow-macos\n!pip install keras","metadata":{"execution":{"iopub.status.busy":"2023-04-13T14:59:51.438027Z","iopub.execute_input":"2023-04-13T14:59:51.438430Z","iopub.status.idle":"2023-04-13T15:00:07.935308Z","shell.execute_reply.started":"2023-04-13T14:59:51.438394Z","shell.execute_reply":"2023-04-13T15:00:07.933569Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow-macos (from versions: none)\u001b[0m\u001b[31m\n\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow-macos\u001b[0m\u001b[31m\n\u001b[0mRequirement already satisfied: keras in /opt/conda/lib/python3.7/site-packages (2.11.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score\nimport random\nfrom tensorflow.compiler.tf2xla.python import xla","metadata":{"id":"MO8O0AiG9STy","execution":{"iopub.status.busy":"2023-04-13T15:00:07.937987Z","iopub.execute_input":"2023-04-13T15:00:07.938401Z","iopub.status.idle":"2023-04-13T15:00:18.745144Z","shell.execute_reply.started":"2023-04-13T15:00:07.938357Z","shell.execute_reply":"2023-04-13T15:00:18.744098Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"## Load the dataset and visualize the data\n\nWe use the `keras.datasets.mnist.load_data()` utility to directly pull the MNIST dataset\nin the form of `NumPy` arrays. We then arrange it in the form of the train and test\nsplits.\n\nFollowing loading the dataset, we select 4 random samples from within the training set\nand visualize them using `matplotlib.pyplot`.","metadata":{"id":"8qH6C3LW9STz"}},{"cell_type":"code","source":"(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\n\nprint(\"4 Random Training samples and labels\")\nidx1, idx2, idx3, idx4 = random.sample(range(0, x_train.shape[0]), 4)\n\nimg1 = (x_train[idx1], y_train[idx1])\nimg2 = (x_train[idx2], y_train[idx2])\nimg3 = (x_train[idx3], y_train[idx3])\nimg4 = (x_train[idx4], y_train[idx4])\n\nimgs = [img1, img2, img3, img4]\n\nplt.figure(figsize=(10, 10))\n\nfor idx, item in enumerate(imgs):\n    image, label = item[0], item[1]\n    plt.subplot(2, 2, idx + 1)\n    plt.imshow(image, cmap=\"gray\")\n    plt.title(f\"Label : {label}\")\nplt.show()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":624},"id":"_8EkzEhb9STz","outputId":"ec041009-3c68-4ad8-b903-d77fcf1521a0","execution":{"iopub.status.busy":"2023-04-13T15:00:18.746764Z","iopub.execute_input":"2023-04-13T15:00:18.747886Z","iopub.status.idle":"2023-04-13T15:00:22.657080Z","shell.execute_reply.started":"2023-04-13T15:00:18.747844Z","shell.execute_reply":"2023-04-13T15:00:22.655614Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n29515/29515 [==============================] - 0s 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n26421880/26421880 [==============================] - 1s 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n5148/5148 [==============================] - 0s 0us/step\nDownloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n4422102/4422102 [==============================] - 1s 0us/step\n4 Random Training samples and labels\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1000x1000 with 4 Axes>","image/png":"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\n"},"metadata":{}}]},{"cell_type":"markdown","source":"## Define `FFDense` custom layer\n\nIn this custom layer, we have a base `keras.layers.Dense` object which acts as the\nbase `Dense` layer within. Since weight updates will happen within the layer itself, we\nadd an `keras.optimizers.Optimizer` object that is accepted from the user. Here, we\nuse `Adam` as our optimizer with a rather higher learning rate of `0.03`.\n\nFollowing the algorithm's specifics, we must set a `threshold` parameter that will be\nused to make the positive-negative decision in each prediction. This is set to a default\nof 2.0.\nAs the epochs are localized to the layer itself, we also set a `num_epochs` parameter\n(defaults to 50).\n\nWe override the `call` method in order to perform a normalization over the complete\ninput space followed by running it through the base `Dense` layer as would happen in a\nnormal `Dense` layer call.\n\nWe implement the Forward-Forward algorithm which accepts 2 kinds of input tensors, each\nrepresenting the positive and negative samples respectively. We write a custom training\nloop here with the use of `tf.GradientTape()`, within which we calculate a loss per\nsample by taking the distance of the prediction from the threshold to understand the\nerror and taking its mean to get a `mean_loss` metric.\n\nWith the help of `tf.GradientTape()` we calculate the gradient updates for the trainable\nbase `Dense` layer and apply them using the layer's local optimizer.\n\nFinally, we return the `call` result as the `Dense` results of the positive and negative\nsamples while also returning the last `mean_loss` metric and all the loss values over a\ncertain all-epoch run.","metadata":{"id":"LDf2NNyA9ST0"}},{"cell_type":"code","source":"\nclass FFDense(keras.layers.Layer):\n    \"\"\"\n    A custom ForwardForward-enabled Dense layer. It has an implementation of the\n    Forward-Forward network internally for use.\n    This layer must be used in conjunction with the `FFNetwork` model.\n    \"\"\"\n\n    def __init__(\n        self,\n        units,\n        optimizer,\n        loss_metric,\n        num_epochs=60,\n        use_bias=True,\n        kernel_initializer=\"glorot_uniform\",\n        bias_initializer=\"zeros\",\n        kernel_regularizer=None,\n        bias_regularizer=None,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n        self.dense = keras.layers.Dense(\n            units=units,\n            use_bias=use_bias,\n            kernel_initializer=kernel_initializer,\n            bias_initializer=bias_initializer,\n            kernel_regularizer=kernel_regularizer,\n            bias_regularizer=bias_regularizer,\n        )\n        self.relu = keras.layers.ReLU()\n        self.optimizer = optimizer\n        self.loss_metric = loss_metric\n        self.threshold = 1.5\n        self.num_epochs = num_epochs\n\n    # We perform a normalization step before we run the input through the Dense\n    # layer.\n\n    def call(self, x):\n        x_norm = tf.norm(x, ord=2, axis=1, keepdims=True)\n        x_norm = x_norm + 1e-4\n        x_dir = x / x_norm\n        res = self.dense(x_dir)\n        return self.relu(res)\n\n    # The Forward-Forward algorithm is below. We first perform the Dense-layer\n    # operation and then get a Mean Square value for all positive and negative\n    # samples respectively.\n    # The custom loss function finds the distance between the Mean-squared\n    # result and the threshold value we set (a hyperparameter) that will define\n    # whether the prediction is positive or negative in nature. Once the loss is\n    # calculated, we get a mean across the entire batch combined and perform a\n    # gradient calculation and optimization step. This does not technically\n    # qualify as backpropagation since there is no gradient being\n    # sent to any previous layer and is completely local in nature.\n\n    def forward_forward(self, x_pos, x_neg):\n        for i in range(self.num_epochs):\n            with tf.GradientTape() as tape:\n                g_pos = tf.math.reduce_mean(tf.math.pow(self.call(x_pos), 2), 1)\n                g_neg = tf.math.reduce_mean(tf.math.pow(self.call(x_neg), 2), 1)\n\n                loss = tf.math.log(\n                    1\n                    + tf.math.exp(\n                        tf.concat([-g_pos + self.threshold, g_neg - self.threshold], 0)\n                    )\n                )\n                mean_loss = tf.cast(tf.math.reduce_mean(loss), tf.float32)\n                self.loss_metric.update_state([mean_loss])\n            gradients = tape.gradient(mean_loss, self.dense.trainable_weights)\n            self.optimizer.apply_gradients(zip(gradients, self.dense.trainable_weights))\n        return (\n            tf.stop_gradient(self.call(x_pos)),\n            tf.stop_gradient(self.call(x_neg)),\n            self.loss_metric.result(),\n        )","metadata":{"id":"p9ZD0Qf-9ST0","execution":{"iopub.status.busy":"2023-04-13T15:00:22.661039Z","iopub.execute_input":"2023-04-13T15:00:22.661514Z","iopub.status.idle":"2023-04-13T15:00:22.903129Z","shell.execute_reply.started":"2023-04-13T15:00:22.661473Z","shell.execute_reply":"2023-04-13T15:00:22.901645Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"## Define the `FFNetwork` Custom Model\n\nWith our custom layer defined, we also need to override the `train_step` method and\ndefine a custom `keras.models.Model` that works with our `FFDense` layer.\n\nFor this algorithm, we must 'embed' the labels onto the original image. To do so, we\nexploit the structure of MNIST images where the top-left 10 pixels are always zeros. We\nuse that as a label space in order to visually one-hot-encode the labels within the image\nitself. This action is performed by the `overlay_y_on_x` function.\n\nWe break down the prediction function with a per-sample prediction function which is then\ncalled over the entire test set by the overriden `predict()` function. The prediction is\nperformed here with the help of measuring the `excitation` of the neurons per layer for\neach image. This is then summed over all layers to calculate a network-wide 'goodness\nscore'. The label with the highest 'goodness score' is then chosen as the sample\nprediction.\n\nThe `train_step` function is overriden to act as the main controlling loop for running\ntraining on each layer as per the number of epochs per layer.","metadata":{"id":"wbwtEHx59ST1"}},{"cell_type":"code","source":"\nclass FFNetwork(keras.Model):\n    def __init__(self,dims,layer_optimizer=keras.optimizers.legacy.Adam(learning_rate=0.03),**kwargs,):\n        super().__init__(**kwargs)\n        self.layer_optimizer = layer_optimizer\n        self.loss_var = tf.Variable(0.0, trainable=False, dtype=tf.float32)\n        self.loss_count = tf.Variable(0.0, trainable=False, dtype=tf.float32)\n        self.layer_list = [keras.Input(shape=(dims[0],))]\n\n        for d in range(3):  #len(dims) - 1\n            self.layer_list += [ FFDense(200,\n                    optimizer=self.layer_optimizer,\n                    loss_metric=keras.metrics.Mean(),\n                )\n            ]\n\n    # This function makes a dynamic change to the image wherein the labels are\n    # put on top of the original image (for this example, as MNIST has 10\n    # unique labels, we take the top-left corner's first 10 pixels). This\n    # function returns the original data tensor with the first 10 pixels being\n    # a pixel-based one-hot representation of the labels.\n\n    @tf.function(reduce_retracing=True)\n    def overlay_y_on_x(self, data):\n        X_sample, y_sample = data\n        max_sample = tf.reduce_max(X_sample, axis=0, keepdims=True)\n        max_sample = tf.cast(max_sample, dtype=tf.float64)\n        X_zeros = tf.zeros([10], dtype=tf.float64)\n        X_update = xla.dynamic_update_slice(X_zeros, max_sample, [y_sample])\n        X_sample = xla.dynamic_update_slice(X_sample, X_update, [0])\n        return X_sample, y_sample\n\n    # A custom `predict_one_sample` performs predictions by passing the images\n    # through the network, measures the results produced by each layer (i.e.\n    # how high/low the output values are with respect to the set threshold for\n    # each label) and then simply finding the label with the highest values.\n    # In such a case, the images are tested for their 'goodness' with all\n    # labels.\n\n    @tf.function(reduce_retracing=True)\n    def predict_one_sample(self, x):\n        goodness_per_label = []\n        x = tf.reshape(x, [tf.shape(x)[0] * tf.shape(x)[1]])\n        for label in range(10):\n            h, label = self.overlay_y_on_x(data=(x, label))\n            h = tf.reshape(h, [-1, tf.shape(h)[0]])\n            goodness = []\n            for layer_idx in range(1, len(self.layer_list)):\n                layer = self.layer_list[layer_idx]\n                h = layer(h)\n                goodness += [tf.math.reduce_mean(tf.math.pow(h, 2), 1)]\n            goodness_per_label += [\n                tf.expand_dims(tf.reduce_sum(goodness, keepdims=True), 1)\n            ]\n        goodness_per_label = tf.concat(goodness_per_label, 1)\n        return tf.cast(tf.argmax(goodness_per_label, 1), tf.float64)\n\n    def predict(self, data):\n        x = data\n        preds = list()\n        preds = tf.map_fn(fn=self.predict_one_sample, elems=x)\n        return np.asarray(preds, dtype=int)\n\n    # This custom `train_step` function overrides the internal `train_step`\n    # implementation. We take all the input image tensors, flatten them and\n    # subsequently produce positive and negative samples on the images.\n    # A positive sample is an image that has the right label encoded on it with\n    # the `overlay_y_on_x` function. A negative sample is an image that has an\n    # erroneous label present on it.\n    # With the samples ready, we pass them through each `FFLayer` and perform\n    # the Forward-Forward computation on it. The returned loss is the final\n    # loss value over all the layers.\n\n    @tf.function(jit_compile=True)\n    def train_step(self, data):\n        x, y = data\n\n        # Flatten op\n        x = tf.reshape(x, [-1, tf.shape(x)[1] * tf.shape(x)[2]])\n\n        x_pos, y = tf.map_fn(fn=self.overlay_y_on_x, elems=(x, y))\n\n        random_y = tf.random.shuffle(y)\n        x_neg, y = tf.map_fn(fn=self.overlay_y_on_x, elems=(x, random_y))\n\n        h_pos, h_neg = x_pos, x_neg\n\n        for idx, layer in enumerate(self.layers):\n            if isinstance(layer, FFDense):\n                print(f\"Training layer {idx+1} now : \")\n                h_pos, h_neg, loss = layer.forward_forward(h_pos, h_neg)\n                self.loss_var.assign_add(loss)\n                self.loss_count.assign_add(1.0)\n            else:\n                print(f\"Passing layer {idx+1} now : \")\n                x = layer(x)\n        mean_res = tf.math.divide(self.loss_var, self.loss_count)\n        return {\"FinalLoss\": mean_res}\n","metadata":{"id":"hEIyo87H9ST1","execution":{"iopub.status.busy":"2023-04-13T15:00:22.905513Z","iopub.execute_input":"2023-04-13T15:00:22.906382Z","iopub.status.idle":"2023-04-13T15:00:22.945508Z","shell.execute_reply.started":"2023-04-13T15:00:22.906326Z","shell.execute_reply":"2023-04-13T15:00:22.944388Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Convert MNIST `NumPy` arrays to `tf.data.Dataset`\n\nWe now perform some preliminary processing on the `NumPy` arrays and then convert them\ninto the `tf.data.Dataset` format which allows for optimized loading.","metadata":{"id":"h-kpGNqx9ST2"}},{"cell_type":"code","source":"x_train = x_train.astype(float) / 255\nx_test = x_test.astype(float) / 255\ny_train = y_train.astype(int)\ny_test = y_test.astype(int)\n\ntrain_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\ntest_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n\ntrain_dataset = train_dataset.batch(60000)\ntest_dataset = test_dataset.batch(10000)","metadata":{"id":"qqht_XWA9ST2","execution":{"iopub.status.busy":"2023-04-13T15:00:22.947392Z","iopub.execute_input":"2023-04-13T15:00:22.948179Z","iopub.status.idle":"2023-04-13T15:00:23.887341Z","shell.execute_reply.started":"2023-04-13T15:00:22.948127Z","shell.execute_reply":"2023-04-13T15:00:23.886031Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# find the dimensions of our images\nimg_width, img_height = x_train.shape[1], x_train.shape[2]\nprint(f\"Image dimensions are {img_width} x {img_height}\")","metadata":{"execution":{"iopub.status.busy":"2023-04-13T15:00:23.889928Z","iopub.execute_input":"2023-04-13T15:00:23.890748Z","iopub.status.idle":"2023-04-13T15:00:23.900406Z","shell.execute_reply.started":"2023-04-13T15:00:23.890689Z","shell.execute_reply":"2023-04-13T15:00:23.898583Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Image dimensions are 28 x 28\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Fit the network and visualize results\n\nHaving performed all previous set-up, we are now going to run `model.fit()` and run 250\nmodel epochs, which will perform 50*250 epochs on each layer. We get to see the plotted loss\ncurve as each layer is trained.","metadata":{"id":"0oatKqiQ9ST2"}},{"cell_type":"code","source":"def custom_loss(y_true, y_pred):\n    return K.mean(K.square(y_pred-y_true)/K.clip(K.square(0.01*y_pred), K.epsilon(), None), axis=-1)","metadata":{"id":"YMGoB0xMPGcl","execution":{"iopub.status.busy":"2023-04-13T15:00:23.902690Z","iopub.execute_input":"2023-04-13T15:00:23.903260Z","iopub.status.idle":"2023-04-13T15:00:23.913234Z","shell.execute_reply.started":"2023-04-13T15:00:23.903197Z","shell.execute_reply":"2023-04-13T15:00:23.911356Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"# model = FFNetwork(dims=[400, 250, 250])\n\n# model.compile(\n#     optimizer=keras.optimizers.Adam(learning_rate=0.06),\n#     loss= custom_loss, #\"mse\",\n#     jit_compile=True,\n#     metrics=[keras.metrics.Mean()],\n# )\n\n# epochs = 300\n# history = model.fit(train_dataset, epochs=epochs)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RUR8wEzO9ST2","outputId":"937f8b4b-fb57-45ee-d2bd-bb1798db181e","execution":{"iopub.status.busy":"2023-04-13T15:00:23.916009Z","iopub.execute_input":"2023-04-13T15:00:23.916679Z","iopub.status.idle":"2023-04-13T15:00:23.924735Z","shell.execute_reply.started":"2023-04-13T15:00:23.916622Z","shell.execute_reply":"2023-04-13T15:00:23.923313Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## Perform inference and testing\n\nHaving trained the model to a large extent, we now see how it performs on the\ntest set. We calculate the Accuracy Score to understand the results closely.","metadata":{"id":"vtZBSfSP9ST2"}},{"cell_type":"code","source":"# preds = model.predict(tf.convert_to_tensor(x_test))\n\n# preds = preds.reshape((preds.shape[0], preds.shape[1]))\n\n# results = accuracy_score(preds, y_test)\n\n# print(f\"Test Accuracy score : {results*100}%\")\n\n# plt.plot(range(len(history.history[\"FinalLoss\"])), history.history[\"FinalLoss\"])\n# plt.title(\"Loss over training\")\n# plt.show()","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":298},"id":"HUjVROwg9ST2","outputId":"4cc47107-a21b-4b15-ab8e-f1ed7c6e8449","execution":{"iopub.status.busy":"2023-04-13T15:00:23.927232Z","iopub.execute_input":"2023-04-13T15:00:23.928279Z","iopub.status.idle":"2023-04-13T15:00:23.936771Z","shell.execute_reply.started":"2023-04-13T15:00:23.928195Z","shell.execute_reply":"2023-04-13T15:00:23.935640Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## Conclusion\n\nThis example has hereby demonstrated how the Forward-Forward algorithm works using\nthe TensorFlow and Keras packages. While the investigation results presented by Prof. Hinton\nin their paper are currently still limited to smaller models and datasets like MNIST and\nFashion-MNIST, subsequent results on larger models like LLMs are expected in future\npapers.\n\nThrough the paper, Prof. Hinton has reported results of 1.36% test accuracy error with a\n2000-units, 4 hidden-layer, fully-connected network run over 60 epochs (while mentioning\nthat backpropagation takes only 20 epochs to achieve similar performance). Another run of\ndoubling the learning rate and training for 40 epochs yields a slightly worse error rate\nof 1.46%\n\nThe current example does not yield state-of-the-art results. But with proper tuning of\nthe Learning Rate, model architecture (number of units in `Dense` layers, kernel\nactivations, initializations, regularization etc.), the results can be improved\nto match the claims of the paper.","metadata":{"id":"CxcvNEgV9ST3"}},{"cell_type":"markdown","source":"## Experiment : Try to improve the results by creating the negative data differently","metadata":{}},{"cell_type":"code","source":"\n\nclass FFNetwork_blur(keras.Model):\n    def __init__(self, dims, layer_optimizer=keras.optimizers.legacy.Adam(learning_rate=0.03), **kwargs,):\n        super().__init__(**kwargs)\n        self.layer_optimizer = layer_optimizer\n        self.loss_var = tf.Variable(0.0, trainable=False, dtype=tf.float32)\n        self.loss_count = tf.Variable(0.0, trainable=False, dtype=tf.float32)\n        self.layer_list = [keras.Input(shape=(dims[0],))]\n\n        for d in range(3):  # len(dims) - 1\n            self.layer_list += [FFDense(200,\n                                        optimizer=self.layer_optimizer,\n                                        loss_metric=keras.metrics.Mean(),\n                                        )\n                                ]\n\n    # This function makes a dynamic change to the image wherein the labels are\n    # put on top of the original image (for this example, as MNIST has 10\n    # unique labels, we take the top-left corner's first 10 pixels). This\n    # function returns the original data tensor with the first 10 pixels being\n    # a pixel-based one-hot representation of the labels.\n\n    @tf.function(reduce_retracing=True)\n    def overlay_y_on_x(self, data):\n        X_sample, y_sample = data\n        max_sample = tf.reduce_max(X_sample, axis=0, keepdims=True)\n        max_sample = tf.cast(max_sample, dtype=tf.float64)\n        X_zeros = tf.zeros([10], dtype=tf.float64)\n        X_update = xla.dynamic_update_slice(X_zeros, max_sample, [y_sample])\n        X_sample = xla.dynamic_update_slice(X_sample, X_update, [0])\n        return X_sample, y_sample\n\n    # A custom `predict_one_sample` performs predictions by passing the images\n    # through the network, measures the results produced by each layer (i.e.\n    # how high/low the output values are with respect to the set threshold for\n    # each label) and then simply finding the label with the highest values.\n    # In such a case, the images are tested for their 'goodness' with all\n    # labels.\n\n    @tf.function(reduce_retracing=True)\n    def predict_one_sample(self, x):\n        goodness_per_label = []\n        x = tf.reshape(x, [tf.shape(x)[0] * tf.shape(x)[1]])\n        for label in range(10):\n            h, label = self.overlay_y_on_x(data=(x, label))\n            h = tf.reshape(h, [-1, tf.shape(h)[0]])\n            goodness = []\n            for layer_idx in range(1, len(self.layer_list)):\n                layer = self.layer_list[layer_idx]\n                h = layer(h)\n                goodness += [tf.math.reduce_mean(tf.math.pow(h, 2), 1)]\n            goodness_per_label += [\n                tf.expand_dims(tf.reduce_sum(goodness, keepdims=True), 1)\n            ]\n        goodness_per_label = tf.concat(goodness_per_label, 1)\n        return tf.cast(tf.argmax(goodness_per_label, 1), tf.float64)\n\n    def predict(self, data):\n        x = data\n        preds = list()\n        preds = tf.map_fn(fn=self.predict_one_sample, elems=x)\n        return np.asarray(preds, dtype=int)\n\n    # This custom `train_step` function overrides the internal `train_step`\n    # implementation. We take all the input image tensors, flatten them and\n    # subsequently produce positive and negative samples on the images.\n    # A positive sample is an image that has the right label encoded on it with\n    # the `overlay_y_on_x` function. A negative sample is an image that has an\n    # erroneous label present on it.\n    # With the samples ready, we pass them through each `FFLayer` and perform\n    # the Forward-Forward computation on it. The returned loss is the final\n    # loss value over all the layers.\n\n    @tf.function(jit_compile=True)\n    def train_step(self, data):\n        x, y = data\n\n        # Flatten op\n        x = tf.reshape(x, [-1, tf.shape(x)[1] * tf.shape(x)[2]])\n\n        x_pos, y = tf.map_fn(fn=self.overlay_y_on_x, elems=(x, y))\n\n        random_y = tf.random.shuffle(y)\n        x_neg, y = tf.map_fn(fn=self.overlay_y_on_x, elems=(x, random_y))\n        # add noise to x_neg\n        x_neg = x_neg + \\\n            tf.random.normal(shape=tf.shape(x_neg), mean=0.0,\n                             stddev=0.1, dtype=tf.float64)\n        # data augment x_neg to create variations using color jitter \n        x_neg = tf.image.random_brightness(x_neg, max_delta=0.5)\n\n        h_pos, h_neg = x_pos, x_neg\n\n        for idx, layer in enumerate(self.layers):\n            if isinstance(layer, FFDense):\n                print(f\"Training layer {idx+1} now \")\n                h_pos, h_neg, loss = layer.forward_forward(h_pos, h_neg)\n                self.loss_var.assign_add(loss)\n                self.loss_count.assign_add(1.0)\n            else:\n                print(f\"Passing layer {idx+1} now \")\n                x = layer(x)\n        mean_res = tf.math.divide(self.loss_var, self.loss_count)\n        return {\"FinalLoss\": mean_res}\n","metadata":{"execution":{"iopub.status.busy":"2023-04-13T15:00:23.941314Z","iopub.execute_input":"2023-04-13T15:00:23.942192Z","iopub.status.idle":"2023-04-13T15:00:23.968497Z","shell.execute_reply.started":"2023-04-13T15:00:23.942140Z","shell.execute_reply":"2023-04-13T15:00:23.966536Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# write me afunction to blur the image\ndef blur_image(image):\n    image = image.reshape((28, 28))\n    image = cv2.GaussianBlur(image, (5, 5), 0)\n    image = image.reshape((1, 28, 28))\n    return image","metadata":{"execution":{"iopub.status.busy":"2023-04-13T15:00:23.970779Z","iopub.execute_input":"2023-04-13T15:00:23.971599Z","iopub.status.idle":"2023-04-13T15:00:23.983926Z","shell.execute_reply.started":"2023-04-13T15:00:23.971526Z","shell.execute_reply":"2023-04-13T15:00:23.982732Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"from tensorflow.python.ops.numpy_ops import np_config\nnp_config.enable_numpy_behavior()\n","metadata":{"execution":{"iopub.status.busy":"2023-04-13T15:00:23.985225Z","iopub.execute_input":"2023-04-13T15:00:23.986343Z","iopub.status.idle":"2023-04-13T15:00:23.998323Z","shell.execute_reply.started":"2023-04-13T15:00:23.986298Z","shell.execute_reply":"2023-04-13T15:00:23.997108Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# # create the model\n# model = FFNetwork_blur(dims=[400, 250, 250])\n\n# # compile the model\n# model.compile(\n#     optimizer=keras.optimizers.Adam(learning_rate=0.06),\n#     loss= custom_loss,\n#     jit_compile=True,\n#     metrics=[keras.metrics.Mean()],\n# )\n\n# # train the model\n# epochs = 300\n# history = model.fit(train_dataset, epochs=epochs)","metadata":{"execution":{"iopub.status.busy":"2023-04-13T15:00:24.000356Z","iopub.execute_input":"2023-04-13T15:00:24.001263Z","iopub.status.idle":"2023-04-13T15:00:24.013643Z","shell.execute_reply.started":"2023-04-13T15:00:24.001191Z","shell.execute_reply":"2023-04-13T15:00:24.012150Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"# # test the model\n# preds = model.predict(tf.convert_to_tensor(x_test))\n# preds = preds.reshape((preds.shape[0], preds.shape[1]))\n# results = accuracy_score(preds, y_test)\n# print(f\"Test Accuracy score : {results*100}%\")\n\n# # compute the error rate \n# error_rate = 1 - results\n\n# print(f\"Test error : {error_rate*100}%\")\n# # plot the loss\n# plt.plot(range(len(history.history[\"FinalLoss\"])), history.history[\"FinalLoss\"])\n# plt.title(\"Loss over training\")\n# plt.show()\n\n","metadata":{"execution":{"iopub.status.busy":"2023-04-13T15:00:24.015332Z","iopub.execute_input":"2023-04-13T15:00:24.016224Z","iopub.status.idle":"2023-04-13T15:00:24.038180Z","shell.execute_reply.started":"2023-04-13T15:00:24.016140Z","shell.execute_reply":"2023-04-13T15:00:24.036655Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"\n\nclass FFNetwork_blur(keras.Model):\n    def __init__(self, dims, layer_optimizer=keras.optimizers.legacy.Adam(learning_rate=0.03), **kwargs,):\n        super().__init__(**kwargs)\n        self.layer_optimizer = layer_optimizer\n        self.loss_var = tf.Variable(0.0, trainable=False, dtype=tf.float32)\n        self.loss_count = tf.Variable(0.0, trainable=False, dtype=tf.float32)\n        self.layer_list = [keras.Input(shape=(dims[0],))]\n\n        for d in range(3):  # len(dims) - 1\n            self.layer_list += [FFDense(200,\n                                        optimizer=self.layer_optimizer,\n                                        loss_metric=keras.metrics.Mean(),\n                                        )\n                                ]\n\n    # This function makes a dynamic change to the image wherein the labels are\n    # put on top of the original image (for this example, as MNIST has 10\n    # unique labels, we take the top-left corner's first 10 pixels). This\n    # function returns the original data tensor with the first 10 pixels being\n    # a pixel-based one-hot representation of the labels.\n\n    @tf.function(reduce_retracing=True)\n    def overlay_y_on_x(self, data):\n        X_sample, y_sample = data\n        max_sample = tf.reduce_max(X_sample, axis=0, keepdims=True)\n        max_sample = tf.cast(max_sample, dtype=tf.float64)\n        X_zeros = tf.zeros([10], dtype=tf.float64)\n        X_update = xla.dynamic_update_slice(X_zeros, max_sample, [y_sample])\n        X_sample = xla.dynamic_update_slice(X_sample, X_update, [0])\n        return X_sample, y_sample\n\n    # A custom `predict_one_sample` performs predictions by passing the images\n    # through the network, measures the results produced by each layer (i.e.\n    # how high/low the output values are with respect to the set threshold for\n    # each label) and then simply finding the label with the highest values.\n    # In such a case, the images are tested for their 'goodness' with all\n    # labels.\n\n    @tf.function(reduce_retracing=True)\n    def predict_one_sample(self, x):\n        goodness_per_label = []\n        x = tf.reshape(x, [tf.shape(x)[0] * tf.shape(x)[1]])\n        for label in range(10):\n            h, label = self.overlay_y_on_x(data=(x, label))\n            h = tf.reshape(h, [-1, tf.shape(h)[0]])\n            goodness = []\n            for layer_idx in range(1, len(self.layer_list)):\n                layer = self.layer_list[layer_idx]\n                h = layer(h)\n                goodness += [tf.math.reduce_mean(tf.math.pow(h, 2), 1)]\n            goodness_per_label += [\n                tf.expand_dims(tf.reduce_sum(goodness, keepdims=True), 1)\n            ]\n        goodness_per_label = tf.concat(goodness_per_label, 1)\n        return tf.cast(tf.argmax(goodness_per_label, 1), tf.float64)\n\n    def predict(self, data):\n        x = data\n        preds = list()\n        preds = tf.map_fn(fn=self.predict_one_sample, elems=x)\n        return np.asarray(preds, dtype=int)\n\n    # This custom `train_step` function overrides the internal `train_step`\n    # implementation. We take all the input image tensors, flatten them and\n    # subsequently produce positive and negative samples on the images.\n    # A positive sample is an image that has the right label encoded on it with\n    # the `overlay_y_on_x` function. A negative sample is an image that has an\n    # erroneous label present on it.\n    # With the samples ready, we pass them through each `FFLayer` and perform\n    # the Forward-Forward computation on it. The returned loss is the final\n    # loss value over all the layers.\n\n    @tf.function(jit_compile=True)\n    def train_step(self, data):\n        x, y = data\n\n        # Flatten op\n        x = tf.reshape(x, [-1, tf.shape(x)[1] * tf.shape(x)[2]])\n\n        x_pos, y = tf.map_fn(fn=self.overlay_y_on_x, elems=(x, y))\n\n        random_y = tf.random.shuffle(y)\n        x_neg, y = tf.map_fn(fn=self.overlay_y_on_x, elems=(x, random_y))\n        # add noise to x_neg\n        x_neg = x_neg + \\\n            tf.random.normal(shape=tf.shape(x_neg), mean=0.0,\n                             stddev=0.1, dtype=tf.float64)\n        # data augment x_neg to create variations using color jitter\n        x_neg = tf.image.random_brightness(x_neg, max_delta=0.5)\n\n        h_pos, h_neg = x_pos, x_neg\n\n        for idx, layer in enumerate(self.layers):\n            if isinstance(layer, FFDense):\n                print(f\"Training layer {idx+1} now \")\n                h_pos, h_neg, loss = layer.forward_forward(h_pos, h_neg)\n                self.loss_var.assign_add(loss)\n                self.loss_count.assign_add(1.0)\n            else:\n                print(f\"Passing layer {idx+1} now \")\n                x = layer(x)\n        mean_res = tf.math.divide(self.loss_var, self.loss_count)\n        return {\"FinalLoss\": mean_res}\n","metadata":{"execution":{"iopub.status.busy":"2023-04-13T15:00:24.040496Z","iopub.execute_input":"2023-04-13T15:00:24.041419Z","iopub.status.idle":"2023-04-13T15:00:24.068784Z","shell.execute_reply.started":"2023-04-13T15:00:24.041348Z","shell.execute_reply":"2023-04-13T15:00:24.067507Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"\nclass FFconv(keras.layers.Layer):\n    \"\"\"\n    A custom ForwardForward-enabled convolutional layer.\n\n    \"\"\"\n\n    def __init__(\n        self,\n        units,\n        num_filters,\n        optimizer,\n        loss_metric,\n        num_epochs=60,\n        use_bias=True,\n        kernel_initializer=\"glorot_uniform\",\n        bias_initializer=\"zeros\",\n        kernel_regularizer=None,\n        kernel_size=(3, 3),\n        bias_regularizer=None,\n        **kwargs,\n    ):\n        super().__init__(**kwargs)\n        self.dense = keras.layers.Dense(\n            units=units,\n            use_bias=use_bias,\n            kernel_initializer=kernel_initializer,\n            bias_initializer=bias_initializer,\n            kernel_regularizer=kernel_regularizer,\n            bias_regularizer=bias_regularizer,\n        )\n        self.relu = keras.layers.ReLU()\n        self.optimizer = optimizer\n        self.loss_metric = loss_metric\n        self.threshold = 1.5\n        self.num_epochs = num_epochs\n\n        # we do the same for the convolutional layer\n\n        self.conv = keras.layers.Conv2D(\n            filters=num_filters,\n            kernel_size=kernel_size,\n            strides=(1, 1),\n            padding=\"same\",\n            activation=\"relu\",\n            kernel_initializer=\"he_normal\",\n            bias_initializer=\"zeros\",\n        )\n\n\n\n    # We perform a normalization step before we run the input through the Dense\n    # layer.\n\n    def call(self, x): # we adapt to use on the convolutional layer\n    \n        x = self.conv(x)\n        #print(x.shape)\n        x_norm = tf.norm(x, ord=2, axis=1, keepdims=True)\n        x_norm = x_norm + 1e-4\n        x_dir = x / x_norm\n        res = self.relu(x_dir)\n        #print(res.shape)\n        return res\n\n    # The Forward-Forward algorithm is below. We first perform the Dense-layer\n    # operation and then get a Mean Square value for all positive and negative\n    # samples respectively.\n    # The custom loss function finds the distance between the Mean-squared\n    # result and the threshold value we set (a hyperparameter) that will define\n    # whether the prediction is positive or negative in nature. Once the loss is\n    # calculated, we get a mean across the entire batch combined and perform a\n    # gradient calculation and optimization step. This does not technically\n    # qualify as backpropagation since there is no gradient being\n    # sent to any previous layer and is completely local in nature.\n\n    def forward_forward(self, x_pos, x_neg): # adapt to use on the convolutional layer, first the x_pos and x_neg need to be reshaped\n        # we flatten the input\n        #x_pos = tf.reshape(x_pos, [-1, tf.shape(x_pos)[1] * tf.shape(x_pos)[2]])\n        #x_neg = tf.reshape(x_neg, [-1, tf.shape(x_neg)[1] * tf.shape(x_neg)[2]])\n        for i in range(self.num_epochs):\n            with tf.GradientTape() as tape:\n                # g_pos must be calculated for each image, and is the sum of the squared values of the output of the x_pos vector\n                # g_neg must be calculated for each image, and is the sum of the squared values of the output of the x_neg vector\n                g_pos = tf.math.reduce_sum(tf.math.square(self.call(x_pos)), 1)\n                g_neg = tf.math.reduce_sum(tf.math.square(self.call(x_neg)), 1)\n\n                loss = tf.math.log(\n                    1\n                    + tf.math.exp(\n                        tf.concat([-g_pos + self.threshold, g_neg - self.threshold], 0)\n                    )\n                )\n                mean_loss = tf.cast(tf.math.reduce_mean(loss), tf.float32)\n                self.loss_metric.update_state([mean_loss])\n            gradients = tape.gradient(mean_loss, self.dense.trainable_weights)\n            self.optimizer.apply_gradients(zip(gradients, self.dense.trainable_weights))\n        return (\n            tf.stop_gradient(self.call(x_pos)),\n            tf.stop_gradient(self.call(x_neg)),\n            self.loss_metric.result(),\n        )\n","metadata":{"execution":{"iopub.status.busy":"2023-04-13T15:29:26.246032Z","iopub.execute_input":"2023-04-13T15:29:26.246459Z","iopub.status.idle":"2023-04-13T15:29:26.264642Z","shell.execute_reply.started":"2023-04-13T15:29:26.246418Z","shell.execute_reply":"2023-04-13T15:29:26.263190Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"class FFConvModel(keras.Model):\n    \"\"\"\n    A custom ForwardForward-enabled convolutional model.\n\n    \"\"\"\n\n    def __init__(self,dims,loss_metric,optimizer,layer_optimizer=keras.optimizers.legacy.Adam(learning_rate=0.03),num_filters=32,**kwargs):\n\n        super().__init__(**kwargs)\n        self.layer_optimizer = layer_optimizer\n        self.loss_var = tf.Variable(0.0, trainable=False, dtype=tf.float32)\n        self.loss_count = tf.Variable(0.0, trainable=False, dtype=tf.float32)\n        self.layer_list = []\n        self.layer_list.append(\n            FFconv(\n                num_filters=num_filters,\n                units=32,\n                kernel_size=(3, 3),\n                optimizer=optimizer,\n                loss_metric=loss_metric,\n                num_epochs=60,\n                name=\"FFconv_1\",\n            )\n        )\n        self.layer_list.append(keras.layers.Flatten())\n        self.layer_list.append(keras.layers.Dense(dims[0], activation=\"softmax\"))\n\n\n    # This function makes a dynamic change to the image wherein the labels are\n    # put on top of the original image (for this example, as MNIST has 10\n    # unique labels, we take the top-left corner's first 10 pixels). This\n    # function returns the original data tensor with the first 10 pixels being\n    # a pixel-based one-hot representation of the labels.\n\n    @tf.function(reduce_retracing=True)\n    def overlay_y_on_x(self, data):\n        X_sample, y_sample = data\n        max_sample = tf.reduce_max(X_sample, axis=0, keepdims=True)\n        max_sample = tf.cast(max_sample, dtype=tf.float64)\n        X_zeros = tf.zeros([10], dtype=tf.float64)\n        X_update = xla.dynamic_update_slice(X_zeros, max_sample, [y_sample])\n        X_sample = xla.dynamic_update_slice(X_sample, X_update, [0])\n        return X_sample, y_sample\n\n    # A custom `predict_one_sample` performs predictions by passing the images\n    # through the network, measures the results produced by each layer (i.e.\n    # how high/low the output values are with respect to the set threshold for\n    # each label) and then simply finding the label with the highest values.\n    # In such a case, the images are tested for their 'goodness' with all\n    # labels.\n\n    @tf.function(reduce_retracing=True)\n    def predict_one_sample(self, x):\n        goodness_per_label = []\n        x = tf.reshape(x, [tf.shape(x)[0] * tf.shape(x)[1]])\n        for label in range(10):\n            h, label = self.overlay_y_on_x(data=(x, label))\n            h = tf.reshape(h, [-1, tf.shape(h)[0]])\n            goodness = []\n            for layer_idx in range(1, len(self.layer_list)):\n                layer = self.layer_list[layer_idx]\n                h = layer(h)\n                \n                goodness += [tf.math.reduce_mean(tf.math.pow(h, 2), 1)]\n            goodness_per_label += [\n                tf.expand_dims(tf.reduce_sum(goodness, keepdims=True), 1)\n            ]\n        goodness_per_label = tf.concat(goodness_per_label, 1)\n        return tf.cast(tf.argmax(goodness_per_label, 1), tf.float64)\n\n    def predict(self, data):\n        x = data\n        preds = list()\n        preds = tf.map_fn(fn=self.predict_one_sample, elems=x)\n        return np.asarray(preds, dtype=int)\n\n    # This custom `train_step` function overrides the internal `train_step`\n    # implementation. We take all the input image tensors, flatten them and\n    # subsequently produce positive and negative samples on the images.\n    # A positive sample is an image that has the right label encoded on it with\n    # the `overlay_y_on_x` function. A negative sample is an image that has an\n    # erroneous label present on it.\n    # With the samples ready, we pass them through each `FFLayer` and perform\n    # the Forward-Forward computation on it. The returned loss is the final\n    # loss value over all the layers.\n\n    @tf.function(jit_compile=True)\n    def train_step(self, data):\n        x, y = data\n        # we store the shape of the input tensor to reshape the output\n        input_shape = tf.shape(x)\n\n        # Flatten op\n        x = tf.reshape(x, [-1, tf.shape(x)[1] * tf.shape(x)[2]])\n        # convert to float64\n        x = tf.cast(x, dtype=tf.float64)\n        x_pos, y = tf.map_fn(fn=self.overlay_y_on_x, elems=(x, y))\n        random_y = tf.random.shuffle(y)\n        x_neg, y = tf.map_fn(fn=self.overlay_y_on_x, elems=(x, random_y))\n        \n        h_pos, h_neg = x_pos, x_neg\n\n        for idx, layer in enumerate(self.layer_list):\n            if isinstance(layer, FFDense):\n                print(f\"Training layer {idx+1} now : \")\n                \n                h_pos, h_neg, loss = layer.forward_forward(h_pos, h_neg)\n                self.loss_var.assign_add(loss)\n                self.loss_count.assign_add(1.0)\n            if isinstance(layer, FFconv):\n                print(f\"Training layer {idx+1} now : \")\n                h_neg = tf.reshape(h_neg, [input_shape[0],input_shape[1],input_shape[2],1])\n                h_pos = tf.reshape(h_pos, [input_shape[0],input_shape[1],input_shape[2],1])\n                h_pos, h_neg, loss = layer.forward_forward(h_pos, h_neg)\n                self.loss_var.assign_add(loss)\n                self.loss_count.assign_add(1.0)\n            else:\n                print(f\"Passing layer {idx+1} now : \")\n                x = layer(x)\n        mean_res = tf.math.divide(self.loss_var, self.loss_count)\n        return {\"FinalLoss\": mean_res}\n","metadata":{"execution":{"iopub.status.busy":"2023-04-13T15:29:29.896682Z","iopub.execute_input":"2023-04-13T15:29:29.897075Z","iopub.status.idle":"2023-04-13T15:29:29.925496Z","shell.execute_reply.started":"2023-04-13T15:29:29.897040Z","shell.execute_reply":"2023-04-13T15:29:29.924102Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"# create the model\n\nloss_metric = keras.metrics.Mean(name=\"loss\")\noptimizer = keras.optimizers.Adam(learning_rate=0.001)\n\nmodel = FFConvModel(dims=[784],loss_metric=loss_metric,optimizer=optimizer)\n\n# compile the model\nmodel.compile(loss=\"sparse_categorical_crossentropy\", optimizer=optimizer, metrics=[\"accuracy\"])","metadata":{"execution":{"iopub.status.busy":"2023-04-13T15:29:33.243648Z","iopub.execute_input":"2023-04-13T15:29:33.244055Z","iopub.status.idle":"2023-04-13T15:29:33.272672Z","shell.execute_reply.started":"2023-04-13T15:29:33.244017Z","shell.execute_reply":"2023-04-13T15:29:33.271296Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(x_train.shape)","metadata":{"execution":{"iopub.status.busy":"2023-04-13T15:29:35.492446Z","iopub.execute_input":"2023-04-13T15:29:35.492938Z","iopub.status.idle":"2023-04-13T15:29:35.501502Z","shell.execute_reply.started":"2023-04-13T15:29:35.492893Z","shell.execute_reply":"2023-04-13T15:29:35.499819Z"},"trusted":true},"execution_count":46,"outputs":[{"name":"stdout","text":"(60000, 28, 28)\n","output_type":"stream"}]},{"cell_type":"code","source":"# train the model\nmodel.fit(x_train, y_train, epochs=10, batch_size=128, validation_split=0.1)","metadata":{"execution":{"iopub.status.busy":"2023-04-13T15:31:56.805068Z","iopub.execute_input":"2023-04-13T15:31:56.806683Z","iopub.status.idle":"2023-04-13T15:32:15.723595Z","shell.execute_reply.started":"2023-04-13T15:31:56.806596Z","shell.execute_reply":"2023-04-13T15:32:15.720821Z"},"trusted":true},"execution_count":48,"outputs":[{"name":"stdout","text":"Epoch 1/10\n422/422 [==============================] - ETA: 0s - FinalLoss: 0.7753","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_27/26878198.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__test_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNotImplementedError\u001b[0m: in user code:\n\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\", line 1820, in test_function  *\n        return step_function(self, iterator)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\", line 1804, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\", line 1792, in run_step  **\n        outputs = model.test_step(data)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\", line 1756, in test_step\n        y_pred = self(x, training=False)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\", line 589, in call\n        \"Unimplemented `tf.keras.Model.call()`: if you \"\n\n    NotImplementedError: Exception encountered when calling layer 'ff_conv_model_6' (type FFConvModel).\n    \n    Unimplemented `tf.keras.Model.call()`: if you intend to create a `Model` with the Functional API, please provide `inputs` and `outputs` arguments. Otherwise, subclass `Model` with an overridden `call()` method.\n    \n    Call arguments received by layer 'ff_conv_model_6' (type FFConvModel):\n      • inputs=tf.Tensor(shape=(None, 28, 28), dtype=float32)\n      • training=False\n      • mask=None\n"],"ename":"NotImplementedError","evalue":"in user code:\n\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\", line 1820, in test_function  *\n        return step_function(self, iterator)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\", line 1804, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\", line 1792, in run_step  **\n        outputs = model.test_step(data)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\", line 1756, in test_step\n        y_pred = self(x, training=False)\n    File \"/opt/conda/lib/python3.7/site-packages/keras/utils/traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"/opt/conda/lib/python3.7/site-packages/keras/engine/training.py\", line 589, in call\n        \"Unimplemented `tf.keras.Model.call()`: if you \"\n\n    NotImplementedError: Exception encountered when calling layer 'ff_conv_model_6' (type FFConvModel).\n    \n    Unimplemented `tf.keras.Model.call()`: if you intend to create a `Model` with the Functional API, please provide `inputs` and `outputs` arguments. Otherwise, subclass `Model` with an overridden `call()` method.\n    \n    Call arguments received by layer 'ff_conv_model_6' (type FFConvModel):\n      • inputs=tf.Tensor(shape=(None, 28, 28), dtype=float32)\n      • training=False\n      • mask=None\n","output_type":"error"}]}]}