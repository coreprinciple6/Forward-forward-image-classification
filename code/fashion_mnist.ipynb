{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MO8O0AiG9STy"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score\n",
        "import random\n",
        "from tensorflow.compiler.tf2xla.python import xla"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qH6C3LW9STz"
      },
      "source": [
        "## Load the dataset and visualize the data\n",
        "\n",
        "We use the `keras.datasets.mnist.load_data()` utility to directly pull the MNIST dataset\n",
        "in the form of `NumPy` arrays. We then arrange it in the form of the train and test\n",
        "splits.\n",
        "\n",
        "Following loading the dataset, we select 4 random samples from within the training set\n",
        "and visualize them using `matplotlib.pyplot`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_8EkzEhb9STz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 624
        },
        "outputId": "ec041009-3c68-4ad8-b903-d77fcf1521a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4 Random Training samples and labels\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 720x720 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "print(\"4 Random Training samples and labels\")\n",
        "idx1, idx2, idx3, idx4 = random.sample(range(0, x_train.shape[0]), 4)\n",
        "\n",
        "img1 = (x_train[idx1], y_train[idx1])\n",
        "img2 = (x_train[idx2], y_train[idx2])\n",
        "img3 = (x_train[idx3], y_train[idx3])\n",
        "img4 = (x_train[idx4], y_train[idx4])\n",
        "\n",
        "imgs = [img1, img2, img3, img4]\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "\n",
        "for idx, item in enumerate(imgs):\n",
        "    image, label = item[0], item[1]\n",
        "    plt.subplot(2, 2, idx + 1)\n",
        "    plt.imshow(image, cmap=\"gray\")\n",
        "    plt.title(f\"Label : {label}\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LDf2NNyA9ST0"
      },
      "source": [
        "## Define `FFDense` custom layer\n",
        "\n",
        "In this custom layer, we have a base `keras.layers.Dense` object which acts as the\n",
        "base `Dense` layer within. Since weight updates will happen within the layer itself, we\n",
        "add an `keras.optimizers.Optimizer` object that is accepted from the user. Here, we\n",
        "use `Adam` as our optimizer with a rather higher learning rate of `0.03`.\n",
        "\n",
        "Following the algorithm's specifics, we must set a `threshold` parameter that will be\n",
        "used to make the positive-negative decision in each prediction. This is set to a default\n",
        "of 2.0.\n",
        "As the epochs are localized to the layer itself, we also set a `num_epochs` parameter\n",
        "(defaults to 50).\n",
        "\n",
        "We override the `call` method in order to perform a normalization over the complete\n",
        "input space followed by running it through the base `Dense` layer as would happen in a\n",
        "normal `Dense` layer call.\n",
        "\n",
        "We implement the Forward-Forward algorithm which accepts 2 kinds of input tensors, each\n",
        "representing the positive and negative samples respectively. We write a custom training\n",
        "loop here with the use of `tf.GradientTape()`, within which we calculate a loss per\n",
        "sample by taking the distance of the prediction from the threshold to understand the\n",
        "error and taking its mean to get a `mean_loss` metric.\n",
        "\n",
        "With the help of `tf.GradientTape()` we calculate the gradient updates for the trainable\n",
        "base `Dense` layer and apply them using the layer's local optimizer.\n",
        "\n",
        "Finally, we return the `call` result as the `Dense` results of the positive and negative\n",
        "samples while also returning the last `mean_loss` metric and all the loss values over a\n",
        "certain all-epoch run."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p9ZD0Qf-9ST0"
      },
      "outputs": [],
      "source": [
        "\n",
        "class FFDense(keras.layers.Layer):\n",
        "    \"\"\"\n",
        "    A custom ForwardForward-enabled Dense layer. It has an implementation of the\n",
        "    Forward-Forward network internally for use.\n",
        "    This layer must be used in conjunction with the `FFNetwork` model.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        units,\n",
        "        optimizer,\n",
        "        loss_metric,\n",
        "        num_epochs=60,\n",
        "        use_bias=True,\n",
        "        kernel_initializer=\"glorot_uniform\",\n",
        "        bias_initializer=\"zeros\",\n",
        "        kernel_regularizer=None,\n",
        "        bias_regularizer=None,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "        self.dense = keras.layers.Dense(\n",
        "            units=units,\n",
        "            use_bias=use_bias,\n",
        "            kernel_initializer=kernel_initializer,\n",
        "            bias_initializer=bias_initializer,\n",
        "            kernel_regularizer=kernel_regularizer,\n",
        "            bias_regularizer=bias_regularizer,\n",
        "        )\n",
        "        self.relu = keras.layers.ReLU()\n",
        "        self.optimizer = optimizer\n",
        "        self.loss_metric = loss_metric\n",
        "        self.threshold = 1.5\n",
        "        self.num_epochs = num_epochs\n",
        "\n",
        "    # We perform a normalization step before we run the input through the Dense\n",
        "    # layer.\n",
        "\n",
        "    def call(self, x):\n",
        "        x_norm = tf.norm(x, ord=2, axis=1, keepdims=True)\n",
        "        x_norm = x_norm + 1e-4\n",
        "        x_dir = x / x_norm\n",
        "        res = self.dense(x_dir)\n",
        "        return self.relu(res)\n",
        "\n",
        "    # The Forward-Forward algorithm is below. We first perform the Dense-layer\n",
        "    # operation and then get a Mean Square value for all positive and negative\n",
        "    # samples respectively.\n",
        "    # The custom loss function finds the distance between the Mean-squared\n",
        "    # result and the threshold value we set (a hyperparameter) that will define\n",
        "    # whether the prediction is positive or negative in nature. Once the loss is\n",
        "    # calculated, we get a mean across the entire batch combined and perform a\n",
        "    # gradient calculation and optimization step. This does not technically\n",
        "    # qualify as backpropagation since there is no gradient being\n",
        "    # sent to any previous layer and is completely local in nature.\n",
        "\n",
        "    def forward_forward(self, x_pos, x_neg):\n",
        "        for i in range(self.num_epochs):\n",
        "            with tf.GradientTape() as tape:\n",
        "                g_pos = tf.math.reduce_mean(tf.math.pow(self.call(x_pos), 2), 1)\n",
        "                g_neg = tf.math.reduce_mean(tf.math.pow(self.call(x_neg), 2), 1)\n",
        "\n",
        "                loss = tf.math.log(\n",
        "                    1\n",
        "                    + tf.math.exp(\n",
        "                        tf.concat([-g_pos + self.threshold, g_neg - self.threshold], 0)\n",
        "                    )\n",
        "                )\n",
        "                mean_loss = tf.cast(tf.math.reduce_mean(loss), tf.float32)\n",
        "                self.loss_metric.update_state([mean_loss])\n",
        "            gradients = tape.gradient(mean_loss, self.dense.trainable_weights)\n",
        "            self.optimizer.apply_gradients(zip(gradients, self.dense.trainable_weights))\n",
        "        return (\n",
        "            tf.stop_gradient(self.call(x_pos)),\n",
        "            tf.stop_gradient(self.call(x_neg)),\n",
        "            self.loss_metric.result(),\n",
        "        )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbwtEHx59ST1"
      },
      "source": [
        "## Define the `FFNetwork` Custom Model\n",
        "\n",
        "With our custom layer defined, we also need to override the `train_step` method and\n",
        "define a custom `keras.models.Model` that works with our `FFDense` layer.\n",
        "\n",
        "For this algorithm, we must 'embed' the labels onto the original image. To do so, we\n",
        "exploit the structure of MNIST images where the top-left 10 pixels are always zeros. We\n",
        "use that as a label space in order to visually one-hot-encode the labels within the image\n",
        "itself. This action is performed by the `overlay_y_on_x` function.\n",
        "\n",
        "We break down the prediction function with a per-sample prediction function which is then\n",
        "called over the entire test set by the overriden `predict()` function. The prediction is\n",
        "performed here with the help of measuring the `excitation` of the neurons per layer for\n",
        "each image. This is then summed over all layers to calculate a network-wide 'goodness\n",
        "score'. The label with the highest 'goodness score' is then chosen as the sample\n",
        "prediction.\n",
        "\n",
        "The `train_step` function is overriden to act as the main controlling loop for running\n",
        "training on each layer as per the number of epochs per layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hEIyo87H9ST1"
      },
      "outputs": [],
      "source": [
        "\n",
        "class FFNetwork(keras.Model):\n",
        "    def __init__(self,dims,layer_optimizer=keras.optimizers.legacy.Adam(learning_rate=0.03),**kwargs,):\n",
        "        super().__init__(**kwargs)\n",
        "        self.layer_optimizer = layer_optimizer\n",
        "        self.loss_var = tf.Variable(0.0, trainable=False, dtype=tf.float32)\n",
        "        self.loss_count = tf.Variable(0.0, trainable=False, dtype=tf.float32)\n",
        "        self.layer_list = [keras.Input(shape=(dims[0],))]\n",
        "\n",
        "        for d in range(3):  #len(dims) - 1\n",
        "            self.layer_list += [ FFDense(200,\n",
        "                    optimizer=self.layer_optimizer,\n",
        "                    loss_metric=keras.metrics.Mean(),\n",
        "                )\n",
        "            ]\n",
        "\n",
        "    # This function makes a dynamic change to the image wherein the labels are\n",
        "    # put on top of the original image (for this example, as MNIST has 10\n",
        "    # unique labels, we take the top-left corner's first 10 pixels). This\n",
        "    # function returns the original data tensor with the first 10 pixels being\n",
        "    # a pixel-based one-hot representation of the labels.\n",
        "\n",
        "    @tf.function(reduce_retracing=True)\n",
        "    def overlay_y_on_x(self, data):\n",
        "        X_sample, y_sample = data\n",
        "        max_sample = tf.reduce_max(X_sample, axis=0, keepdims=True)\n",
        "        max_sample = tf.cast(max_sample, dtype=tf.float64)\n",
        "        X_zeros = tf.zeros([10], dtype=tf.float64)\n",
        "        X_update = xla.dynamic_update_slice(X_zeros, max_sample, [y_sample])\n",
        "        X_sample = xla.dynamic_update_slice(X_sample, X_update, [0])\n",
        "        return X_sample, y_sample\n",
        "\n",
        "    # A custom `predict_one_sample` performs predictions by passing the images\n",
        "    # through the network, measures the results produced by each layer (i.e.\n",
        "    # how high/low the output values are with respect to the set threshold for\n",
        "    # each label) and then simply finding the label with the highest values.\n",
        "    # In such a case, the images are tested for their 'goodness' with all\n",
        "    # labels.\n",
        "\n",
        "    @tf.function(reduce_retracing=True)\n",
        "    def predict_one_sample(self, x):\n",
        "        goodness_per_label = []\n",
        "        x = tf.reshape(x, [tf.shape(x)[0] * tf.shape(x)[1]])\n",
        "        for label in range(10):\n",
        "            h, label = self.overlay_y_on_x(data=(x, label))\n",
        "            h = tf.reshape(h, [-1, tf.shape(h)[0]])\n",
        "            goodness = []\n",
        "            for layer_idx in range(1, len(self.layer_list)):\n",
        "                layer = self.layer_list[layer_idx]\n",
        "                h = layer(h)\n",
        "                goodness += [tf.math.reduce_mean(tf.math.pow(h, 2), 1)]\n",
        "            goodness_per_label += [\n",
        "                tf.expand_dims(tf.reduce_sum(goodness, keepdims=True), 1)\n",
        "            ]\n",
        "        goodness_per_label = tf.concat(goodness_per_label, 1)\n",
        "        return tf.cast(tf.argmax(goodness_per_label, 1), tf.float64)\n",
        "\n",
        "    def predict(self, data):\n",
        "        x = data\n",
        "        preds = list()\n",
        "        preds = tf.map_fn(fn=self.predict_one_sample, elems=x)\n",
        "        return np.asarray(preds, dtype=int)\n",
        "\n",
        "    # This custom `train_step` function overrides the internal `train_step`\n",
        "    # implementation. We take all the input image tensors, flatten them and\n",
        "    # subsequently produce positive and negative samples on the images.\n",
        "    # A positive sample is an image that has the right label encoded on it with\n",
        "    # the `overlay_y_on_x` function. A negative sample is an image that has an\n",
        "    # erroneous label present on it.\n",
        "    # With the samples ready, we pass them through each `FFLayer` and perform\n",
        "    # the Forward-Forward computation on it. The returned loss is the final\n",
        "    # loss value over all the layers.\n",
        "\n",
        "    @tf.function(jit_compile=True)\n",
        "    def train_step(self, data):\n",
        "        x, y = data\n",
        "\n",
        "        # Flatten op\n",
        "        x = tf.reshape(x, [-1, tf.shape(x)[1] * tf.shape(x)[2]])\n",
        "\n",
        "        x_pos, y = tf.map_fn(fn=self.overlay_y_on_x, elems=(x, y))\n",
        "\n",
        "        random_y = tf.random.shuffle(y)\n",
        "        x_neg, y = tf.map_fn(fn=self.overlay_y_on_x, elems=(x, random_y))\n",
        "\n",
        "        h_pos, h_neg = x_pos, x_neg\n",
        "\n",
        "        for idx, layer in enumerate(self.layers):\n",
        "            if isinstance(layer, FFDense):\n",
        "                print(f\"Training layer {idx+1} now : \")\n",
        "                h_pos, h_neg, loss = layer.forward_forward(h_pos, h_neg)\n",
        "                self.loss_var.assign_add(loss)\n",
        "                self.loss_count.assign_add(1.0)\n",
        "            else:\n",
        "                print(f\"Passing layer {idx+1} now : \")\n",
        "                x = layer(x)\n",
        "        mean_res = tf.math.divide(self.loss_var, self.loss_count)\n",
        "        return {\"FinalLoss\": mean_res}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-kpGNqx9ST2"
      },
      "source": [
        "## Convert MNIST `NumPy` arrays to `tf.data.Dataset`\n",
        "\n",
        "We now perform some preliminary processing on the `NumPy` arrays and then convert them\n",
        "into the `tf.data.Dataset` format which allows for optimized loading."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqht_XWA9ST2"
      },
      "outputs": [],
      "source": [
        "x_train = x_train.astype(float) / 255\n",
        "x_test = x_test.astype(float) / 255\n",
        "y_train = y_train.astype(int)\n",
        "y_test = y_test.astype(int)\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "\n",
        "train_dataset = train_dataset.batch(60000)\n",
        "test_dataset = test_dataset.batch(10000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oatKqiQ9ST2"
      },
      "source": [
        "## Fit the network and visualize results\n",
        "\n",
        "Having performed all previous set-up, we are now going to run `model.fit()` and run 250\n",
        "model epochs, which will perform 50*250 epochs on each layer. We get to see the plotted loss\n",
        "curve as each layer is trained."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_loss(y_true, y_pred):\n",
        "    return K.mean(K.square(y_pred-y_true)/K.clip(K.square(0.01*y_pred), K.epsilon(), None), axis=-1)"
      ],
      "metadata": {
        "id": "YMGoB0xMPGcl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RUR8wEzO9ST2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "937f8b4b-fb57-45ee-d2bd-bb1798db181e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300\n",
            "Training layer 1 now : \n",
            "Training layer 2 now : \n",
            "Training layer 3 now : \n",
            "Training layer 1 now : \n",
            "Training layer 2 now : \n",
            "Training layer 3 now : \n",
            "1/1 [==============================] - 148s 148s/step - FinalLoss: 0.7169\n",
            "Epoch 2/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.7010\n",
            "Epoch 3/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.6809\n",
            "Epoch 4/300\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.6644\n",
            "Epoch 5/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.6515\n",
            "Epoch 6/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.6408\n",
            "Epoch 7/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.6321\n",
            "Epoch 8/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.6242\n",
            "Epoch 9/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.6173\n",
            "Epoch 10/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.6112\n",
            "Epoch 11/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.6056\n",
            "Epoch 12/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.6006\n",
            "Epoch 13/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.5960\n",
            "Epoch 14/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.5916\n",
            "Epoch 15/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.5876\n",
            "Epoch 16/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.5838\n",
            "Epoch 17/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.5801\n",
            "Epoch 18/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.5765\n",
            "Epoch 19/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.5731\n",
            "Epoch 20/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.5697\n",
            "Epoch 21/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.5665\n",
            "Epoch 22/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.5634\n",
            "Epoch 23/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.5603\n",
            "Epoch 24/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.5572\n",
            "Epoch 25/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.5544\n",
            "Epoch 26/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.5515\n",
            "Epoch 27/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.5487\n",
            "Epoch 28/300\n",
            "1/1 [==============================] - 5s 5s/step - FinalLoss: 0.5460\n",
            "Epoch 29/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.5435\n",
            "Epoch 30/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.5409\n",
            "Epoch 31/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.5385\n",
            "Epoch 32/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.5361\n",
            "Epoch 33/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.5338\n",
            "Epoch 34/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.5316\n",
            "Epoch 35/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.5293\n",
            "Epoch 36/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.5271\n",
            "Epoch 37/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.5250\n",
            "Epoch 38/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.5229\n",
            "Epoch 39/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.5209\n",
            "Epoch 40/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.5189\n",
            "Epoch 41/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.5170\n",
            "Epoch 42/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.5151\n",
            "Epoch 43/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.5132\n",
            "Epoch 44/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.5114\n",
            "Epoch 45/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.5096\n",
            "Epoch 46/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.5078\n",
            "Epoch 47/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.5061\n",
            "Epoch 48/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.5045\n",
            "Epoch 49/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.5028\n",
            "Epoch 50/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.5012\n",
            "Epoch 51/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4996\n",
            "Epoch 52/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4981\n",
            "Epoch 53/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4966\n",
            "Epoch 54/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4951\n",
            "Epoch 55/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4936\n",
            "Epoch 56/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4921\n",
            "Epoch 57/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4907\n",
            "Epoch 58/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4893\n",
            "Epoch 59/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4879\n",
            "Epoch 60/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4865\n",
            "Epoch 61/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4852\n",
            "Epoch 62/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4839\n",
            "Epoch 63/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4826\n",
            "Epoch 64/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4813\n",
            "Epoch 65/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4801\n",
            "Epoch 66/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4788\n",
            "Epoch 67/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4776\n",
            "Epoch 68/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4764\n",
            "Epoch 69/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4752\n",
            "Epoch 70/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4741\n",
            "Epoch 71/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4730\n",
            "Epoch 72/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4719\n",
            "Epoch 73/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4708\n",
            "Epoch 74/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4697\n",
            "Epoch 75/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4687\n",
            "Epoch 76/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4676\n",
            "Epoch 77/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4666\n",
            "Epoch 78/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4656\n",
            "Epoch 79/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4646\n",
            "Epoch 80/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4636\n",
            "Epoch 81/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4627\n",
            "Epoch 82/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4617\n",
            "Epoch 83/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4607\n",
            "Epoch 84/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4598\n",
            "Epoch 85/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4588\n",
            "Epoch 86/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4579\n",
            "Epoch 87/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4570\n",
            "Epoch 88/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4561\n",
            "Epoch 89/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4552\n",
            "Epoch 90/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4543\n",
            "Epoch 91/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4535\n",
            "Epoch 92/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4526\n",
            "Epoch 93/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4518\n",
            "Epoch 94/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4510\n",
            "Epoch 95/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4502\n",
            "Epoch 96/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4494\n",
            "Epoch 97/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4486\n",
            "Epoch 98/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4478\n",
            "Epoch 99/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4470\n",
            "Epoch 100/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4463\n",
            "Epoch 101/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4455\n",
            "Epoch 102/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4448\n",
            "Epoch 103/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4440\n",
            "Epoch 104/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4433\n",
            "Epoch 105/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4426\n",
            "Epoch 106/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4419\n",
            "Epoch 107/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4412\n",
            "Epoch 108/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4405\n",
            "Epoch 109/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4398\n",
            "Epoch 110/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4391\n",
            "Epoch 111/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4385\n",
            "Epoch 112/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4378\n",
            "Epoch 113/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4372\n",
            "Epoch 114/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4366\n",
            "Epoch 115/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4359\n",
            "Epoch 116/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4353\n",
            "Epoch 117/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4347\n",
            "Epoch 118/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4341\n",
            "Epoch 119/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4335\n",
            "Epoch 120/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4329\n",
            "Epoch 121/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4323\n",
            "Epoch 122/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4317\n",
            "Epoch 123/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4312\n",
            "Epoch 124/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4306\n",
            "Epoch 125/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4300\n",
            "Epoch 126/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4295\n",
            "Epoch 127/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4290\n",
            "Epoch 128/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4284\n",
            "Epoch 129/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4279\n",
            "Epoch 130/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4274\n",
            "Epoch 131/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4268\n",
            "Epoch 132/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4263\n",
            "Epoch 133/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4258\n",
            "Epoch 134/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4253\n",
            "Epoch 135/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4248\n",
            "Epoch 136/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4244\n",
            "Epoch 137/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4239\n",
            "Epoch 138/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4234\n",
            "Epoch 139/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4229\n",
            "Epoch 140/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4224\n",
            "Epoch 141/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4220\n",
            "Epoch 142/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4215\n",
            "Epoch 143/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4211\n",
            "Epoch 144/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4207\n",
            "Epoch 145/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4202\n",
            "Epoch 146/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4198\n",
            "Epoch 147/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4194\n",
            "Epoch 148/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4189\n",
            "Epoch 149/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4185\n",
            "Epoch 150/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4181\n",
            "Epoch 151/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4177\n",
            "Epoch 152/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4173\n",
            "Epoch 153/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4169\n",
            "Epoch 154/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4165\n",
            "Epoch 155/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4161\n",
            "Epoch 156/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4158\n",
            "Epoch 157/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4154\n",
            "Epoch 158/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4150\n",
            "Epoch 159/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4146\n",
            "Epoch 160/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4143\n",
            "Epoch 161/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4139\n",
            "Epoch 162/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4135\n",
            "Epoch 163/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4131\n",
            "Epoch 164/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4128\n",
            "Epoch 165/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4124\n",
            "Epoch 166/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4121\n",
            "Epoch 167/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4117\n",
            "Epoch 168/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4114\n",
            "Epoch 169/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4111\n",
            "Epoch 170/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4107\n",
            "Epoch 171/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4104\n",
            "Epoch 172/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4101\n",
            "Epoch 173/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4098\n",
            "Epoch 174/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4094\n",
            "Epoch 175/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4091\n",
            "Epoch 176/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4088\n",
            "Epoch 177/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4085\n",
            "Epoch 178/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4082\n",
            "Epoch 179/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4079\n",
            "Epoch 180/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4076\n",
            "Epoch 181/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4073\n",
            "Epoch 182/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4070\n",
            "Epoch 183/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4067\n",
            "Epoch 184/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4064\n",
            "Epoch 185/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4061\n",
            "Epoch 186/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4058\n",
            "Epoch 187/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4056\n",
            "Epoch 188/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4053\n",
            "Epoch 189/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4050\n",
            "Epoch 190/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4047\n",
            "Epoch 191/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4045\n",
            "Epoch 192/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4042\n",
            "Epoch 193/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4039\n",
            "Epoch 194/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4037\n",
            "Epoch 195/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4034\n",
            "Epoch 196/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4032\n",
            "Epoch 197/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4029\n",
            "Epoch 198/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4026\n",
            "Epoch 199/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4024\n",
            "Epoch 200/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4021\n",
            "Epoch 201/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4019\n",
            "Epoch 202/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4016\n",
            "Epoch 203/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4014\n",
            "Epoch 204/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4011\n",
            "Epoch 205/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4009\n",
            "Epoch 206/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4007\n",
            "Epoch 207/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4004\n",
            "Epoch 208/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4002\n",
            "Epoch 209/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.4000\n",
            "Epoch 210/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3997\n",
            "Epoch 211/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3995\n",
            "Epoch 212/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3993\n",
            "Epoch 213/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3990\n",
            "Epoch 214/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3988\n",
            "Epoch 215/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3986\n",
            "Epoch 216/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3983\n",
            "Epoch 217/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3981\n",
            "Epoch 218/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3979\n",
            "Epoch 219/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3977\n",
            "Epoch 220/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3975\n",
            "Epoch 221/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3972\n",
            "Epoch 222/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3970\n",
            "Epoch 223/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3968\n",
            "Epoch 224/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3966\n",
            "Epoch 225/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3964\n",
            "Epoch 226/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3962\n",
            "Epoch 227/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3960\n",
            "Epoch 228/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3957\n",
            "Epoch 229/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3956\n",
            "Epoch 230/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3954\n",
            "Epoch 231/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3952\n",
            "Epoch 232/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3950\n",
            "Epoch 233/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3948\n",
            "Epoch 234/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3946\n",
            "Epoch 235/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3944\n",
            "Epoch 236/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3942\n",
            "Epoch 237/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3940\n",
            "Epoch 238/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3938\n",
            "Epoch 239/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3936\n",
            "Epoch 240/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3934\n",
            "Epoch 241/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3932\n",
            "Epoch 242/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3931\n",
            "Epoch 243/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3929\n",
            "Epoch 244/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3927\n",
            "Epoch 245/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3925\n",
            "Epoch 246/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3924\n",
            "Epoch 247/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3922\n",
            "Epoch 248/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3920\n",
            "Epoch 249/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3919\n",
            "Epoch 250/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3917\n",
            "Epoch 251/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3915\n",
            "Epoch 252/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3914\n",
            "Epoch 253/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3912\n",
            "Epoch 254/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3910\n",
            "Epoch 255/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3908\n",
            "Epoch 256/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3907\n",
            "Epoch 257/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3905\n",
            "Epoch 258/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3904\n",
            "Epoch 259/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3902\n",
            "Epoch 260/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3900\n",
            "Epoch 261/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3899\n",
            "Epoch 262/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3897\n",
            "Epoch 263/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3896\n",
            "Epoch 264/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3894\n",
            "Epoch 265/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3893\n",
            "Epoch 266/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3891\n",
            "Epoch 267/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3890\n",
            "Epoch 268/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3888\n",
            "Epoch 269/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3887\n",
            "Epoch 270/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3885\n",
            "Epoch 271/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3884\n",
            "Epoch 272/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3882\n",
            "Epoch 273/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3881\n",
            "Epoch 274/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3879\n",
            "Epoch 275/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3878\n",
            "Epoch 276/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3876\n",
            "Epoch 277/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3875\n",
            "Epoch 278/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3874\n",
            "Epoch 279/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3872\n",
            "Epoch 280/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3871\n",
            "Epoch 281/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3869\n",
            "Epoch 282/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3868\n",
            "Epoch 283/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3866\n",
            "Epoch 284/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3865\n",
            "Epoch 285/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3864\n",
            "Epoch 286/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3862\n",
            "Epoch 287/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3861\n",
            "Epoch 288/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3859\n",
            "Epoch 289/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3858\n",
            "Epoch 290/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3857\n",
            "Epoch 291/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3855\n",
            "Epoch 292/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3854\n",
            "Epoch 293/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3853\n",
            "Epoch 294/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3851\n",
            "Epoch 295/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3850\n",
            "Epoch 296/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3849\n",
            "Epoch 297/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3847\n",
            "Epoch 298/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3846\n",
            "Epoch 299/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3845\n",
            "Epoch 300/300\n",
            "1/1 [==============================] - 4s 4s/step - FinalLoss: 0.3844\n"
          ]
        }
      ],
      "source": [
        "model = FFNetwork(dims=[400, 250, 250])\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=0.06),\n",
        "    loss= custom_loss #\"mse\",\n",
        "    jit_compile=True,\n",
        "    metrics=[keras.metrics.Mean()],\n",
        ")\n",
        "\n",
        "epochs = 300\n",
        "history = model.fit(train_dataset, epochs=epochs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtZBSfSP9ST2"
      },
      "source": [
        "## Perform inference and testing\n",
        "\n",
        "Having trained the model to a large extent, we now see how it performs on the\n",
        "test set. We calculate the Accuracy Score to understand the results closely."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HUjVROwg9ST2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "outputId": "4cc47107-a21b-4b15-ab8e-f1ed7c6e8449"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Accuracy score : 87.44%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "preds = model.predict(tf.convert_to_tensor(x_test))\n",
        "\n",
        "preds = preds.reshape((preds.shape[0], preds.shape[1]))\n",
        "\n",
        "results = accuracy_score(preds, y_test)\n",
        "\n",
        "print(f\"Test Accuracy score : {results*100}%\")\n",
        "\n",
        "plt.plot(range(len(history.history[\"FinalLoss\"])), history.history[\"FinalLoss\"])\n",
        "plt.title(\"Loss over training\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxcvNEgV9ST3"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "This example has hereby demonstrated how the Forward-Forward algorithm works using\n",
        "the TensorFlow and Keras packages. While the investigation results presented by Prof. Hinton\n",
        "in their paper are currently still limited to smaller models and datasets like MNIST and\n",
        "Fashion-MNIST, subsequent results on larger models like LLMs are expected in future\n",
        "papers.\n",
        "\n",
        "Through the paper, Prof. Hinton has reported results of 1.36% test accuracy error with a\n",
        "2000-units, 4 hidden-layer, fully-connected network run over 60 epochs (while mentioning\n",
        "that backpropagation takes only 20 epochs to achieve similar performance). Another run of\n",
        "doubling the learning rate and training for 40 epochs yields a slightly worse error rate\n",
        "of 1.46%\n",
        "\n",
        "The current example does not yield state-of-the-art results. But with proper tuning of\n",
        "the Learning Rate, model architecture (number of units in `Dense` layers, kernel\n",
        "activations, initializations, regularization etc.), the results can be improved\n",
        "to match the claims of the paper."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}